{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1f3af9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "\n",
    "## Define the function to scrape a page\n",
    "def scrape_page(page_link):\n",
    "    r = requests.get(page_link, headers=headers)\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "    entries_block = soup.find(\"ul\", {\"id\": \"entry-item-list\"})\n",
    "    \n",
    "    if not entries_block:\n",
    "        print(f\"No entries found on page: {page_link}\")\n",
    "        return []\n",
    "    \n",
    "    entries_block = entries_block.find_all(\"li\")\n",
    "    page_entries = []\n",
    "    for entry in entries_block:\n",
    "        username = entry.get(\"data-author\")\n",
    "        content = entry.find(\"div\", {\"class\": \"content\"}).text.strip()  # Strip leading/trailing whitespaces\n",
    "        date = entry.find(\"a\", {\"class\": \"entry-date permalink\"}).text\n",
    "        page_entries.append((username, content, date))\n",
    "    \n",
    "    return page_entries\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mainLink = input(\"Please input your URL\")  # Get the link from the user\n",
    "    mainLink = mainLink.split(\"?\")[0]  # If the link has a query string, remove it\n",
    "    \n",
    "    headers = {\n",
    "        \"user-agent\": (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.109 Safari/537.36 OPR/84.0.4316.31\")\n",
    "    }\n",
    "\n",
    "    # Get the page count from the \"data-pagecount\" attribute\n",
    "    r = requests.get(mainLink, headers=headers)\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "    page_count_div = soup.find(\"div\", {\"class\": \"pager\"})\n",
    "    \n",
    "    if not page_count_div:\n",
    "        raise ValueError(\"Could not find the 'pager' div on the page.\")\n",
    "    \n",
    "    pageCount = int(page_count_div.get(\"data-pagecount\"))\n",
    "\n",
    "    # Create an empty list to store all entries\n",
    "    all_entries = []\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        page_links = [mainLink + \"?p=\" + str(i) for i in range(1, pageCount + 1)]\n",
    "        results = executor.map(scrape_page, page_links)\n",
    "        for result in results:\n",
    "            all_entries.extend(result)\n",
    "\n",
    "    # Create DataFrame from scraped data\n",
    "    EntryDF = pd.DataFrame(all_entries, columns=[\"username\", \"entry\", \"date\"])\n",
    "    \n",
    "    # Remove rows with missing or irrelevant data\n",
    "    EntryDF.dropna(inplace=True)\n",
    "    \n",
    "    # Save DataFrame to CSV with proper alignment\n",
    "    EntryDF.to_csv(\"Entries.csv\", encoding=\"utf-8-sig\", index_label=\"Index\")\n",
    "    print(\"Entries successfully retrieved!\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
